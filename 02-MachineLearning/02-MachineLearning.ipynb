{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae284be",
   "metadata": {},
   "source": [
    "# scikit-learn, Machine Learning in Python\n",
    "- Simple and efficient tools for predictive data analysis\n",
    "- Accessible to everybody, and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable - BSD license\n",
    "\n",
    "\n",
    "Check also: \n",
    "- https://scikit-learn.org/stable/index.html\n",
    "- https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932b785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45a8f4",
   "metadata": {},
   "source": [
    "Solution of the previous `Hands on`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(normalize=True):\n",
    "    df = pd.read_csv(\"datasets/housing.csv\", delimiter=\",\")\n",
    "    \n",
    "    num_df = df.drop(columns=[\"ocean_proximity\", \"median_house_value\"])\n",
    "\n",
    "    # Filling in (i.e., impute) missing values with the median value \n",
    "    num_df[\"total_bedrooms\"] = num_df[\"total_bedrooms\"].fillna(num_df[\"total_bedrooms\"].median())\n",
    "\n",
    "    # Add a new column: population_per_household = population / households\n",
    "    num_df[\"population_per_household\"] = num_df[\"population\"] / num_df[\"households\"]\n",
    "\n",
    "    # Add a new column: rooms_per_household = total_rooms / households\n",
    "    num_df[\"rooms_per_household\"] = num_df[\"total_rooms\"] / num_df[\"households\"]\n",
    "\n",
    "    # Add a new column: bedrooms_per_room = total_bedrooms / total_rooms\n",
    "    num_df[\"bedrooms_per_room\"] = num_df[\"total_bedrooms\"] / num_df[\"total_rooms\"]\n",
    "\n",
    "    if normalize:\n",
    "        # Apply standardization to all the numeric columns\n",
    "        num_df = (num_df - num_df.mean()) / num_df.std()\n",
    "\n",
    "    # One hot encode `ocean_proximity` since it is a categorical attribute \n",
    "    cat_df = pd.get_dummies(df[\"ocean_proximity\"], prefix='ocean_proximity')\n",
    "\n",
    "    # Join all the dataframes\n",
    "    return pd.concat([num_df, cat_df, df[[\"median_house_value\"]]], axis=1) # do not change this line\n",
    "\n",
    "df = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc20801",
   "metadata": {},
   "source": [
    "For a supervised learning problem we need:\n",
    "- input data along with labels\n",
    "- split data between test and training set. How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration only. Sklearn has train_test_split()\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data)) # get a list of random numbers\n",
    "    test_set_size = int(len(data) * test_ratio) # define the size of test dataset\n",
    "    test_indices = shuffled_indices[:test_set_size] # the test dataset includes some indexes \n",
    "    train_indices = shuffled_indices[test_set_size:] # the train dataset includes the others (no overlapping)\n",
    "    return data.iloc[train_indices], data.iloc[test_indices] # return the two dataframes\n",
    "\n",
    "df_train, df_test = split_train_test(df, 0.2) # get the train and test dataframes\n",
    "\n",
    "X_train = df_train.drop(columns=[\"median_house_value\"]).to_numpy() # get the train feature matrix\n",
    "X_test  = df_test.drop(columns=[\"median_house_value\"]).to_numpy() # get the test feature matrix\n",
    "y_train = df_train[\"median_house_value\"].to_numpy() # get the train label array\n",
    "y_test  = df_test[\"median_house_value\"].to_numpy() # get the test label array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df[\"median_house_value\"] # labels\n",
    "X = df.drop(columns=[\"median_house_value\"]) # input data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613643ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90facd0d",
   "metadata": {},
   "source": [
    "Apply linear regression to forecast housing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # import the model\n",
    "\n",
    "lin_reg = LinearRegression() # initialize the model (i.e., the estimator)\n",
    "lin_reg.fit(X_train, y_train) # train it\n",
    "housing_predictions = lin_reg.predict(X_test) # predict the cost of houses in the test set\n",
    "\n",
    "lin_mse = mean_squared_error(y_test, housing_predictions) # check the error\n",
    "lin_rmse = np.sqrt(lin_mse) # apply the squared root of the error\n",
    "print(lin_rmse) # print it\n",
    "\n",
    "# visualize some predictions\n",
    "df = pd.DataFrame({'y_test': y_test[:10].to_numpy(), 'y_pred': housing_predictions[:10]}, index=[x for x in range(10)])\n",
    "df.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428764a9",
   "metadata": {},
   "source": [
    "This is better than nothing, but clearly not a great score: most districtsâ€™ `median_housing_values` range between 120K USD and  265K USD, so a typical prediction error of ~70K USD is not very satisfying. This is an example of a model underfitting the training\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109abc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # import the model\n",
    "\n",
    "def run_forest(n_estimators, max_features):\n",
    "    # initialize the model (i.e., the estimator)\n",
    "    forest_reg = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, random_state=42)\n",
    "    forest_reg.fit(X_train, y_train) # train it\n",
    "    housing_predictions = forest_reg.predict(X_test) # predict the cost of houses in the test set\n",
    "    \n",
    "    forest_rmse = mean_squared_error(y_test, housing_predictions, squared=True) # check the error\n",
    "    forest_rmse = np.sqrt(forest_rmse) # apply the squared root of the error\n",
    "    print(forest_rmse)\n",
    "    \n",
    "    df = pd.DataFrame({'y_test': y_test[:10].to_numpy(), 'y_pred': housing_predictions[:10]}, index=[x for x in range(10)])\n",
    "    df.plot.bar(rot=0)\n",
    "    return forest_reg\n",
    "\n",
    "forest_reg = run_forest(100, \"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f5256",
   "metadata": {},
   "source": [
    "Look at parameters used by our current forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0235826",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "n_estimators = [50, 100, 150] # Number of trees in random forest\n",
    "max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "# auto -> n_features\n",
    "# sqrt -> max_features=sqrt(n_features)\n",
    "random_grid = {'n_estimators': n_estimators, 'max_features': max_features}\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random = RandomizedSearchCV(estimator = forest_reg, param_distributions = random_grid, cv = 3, verbose=2, random_state=42)\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_forest(150, \"sqrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f03591",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41167ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X_train.values)\n",
    "\n",
    "# tsne = TSNE(n_components=2)\n",
    "# result = tsne.fit_transform(X_train)\n",
    "\n",
    "plt.scatter(\n",
    "    x=result[:,0], \n",
    "    y=result[:,1] , \n",
    "    c=y_train, #.apply(lambda x: 0 if x == 'low' else (1 if x == 'medium' else 2)), \n",
    "    cmap='viridis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18f9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dump components relations with features\n",
    "pd.DataFrame(pca.components_, columns=X_train.columns, index=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65e48c",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "- Create classes (low, medium, high) from the target value\n",
    "- Create and fit a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c0883",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = preprocess(False) # clean the dataset withtout standardization\n",
    "bins = [\"low\", \"medium\", \"high\"] # the labels we are gonna use\n",
    "df[\"median_house_value\"] = pd.cut(df[\"median_house_value\"], 3, labels=bins) # map bins of values into labels\n",
    "df[\"median_house_value\"].hist() # plot them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a864ea4",
   "metadata": {},
   "source": [
    "For the sake of performance, we are going to use a manually defined set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"median_house_value\"] # labels\n",
    "X = df[[\"latitude\", \"longitude\", \"median_income\", \"population\", \"households\", \"total_bedrooms\"]] # consider only some features from the train dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a910d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=42) # initialize the model\n",
    "clf = clf.fit(X_train, y_train) # train it\n",
    "housing_predictions = clf.predict(X_test) # predict the cost of houses in the test set\n",
    "\n",
    "accuracy = accuracy_score(y_test, housing_predictions) # check the error\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a816a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, housing_predictions, target_names=bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f3c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "tree.plot_tree(clf, fontsize=10, feature_names=X_train.columns, class_names=bins, max_depth=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
