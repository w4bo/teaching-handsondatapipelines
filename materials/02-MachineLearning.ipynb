{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w4bo/2022-bbs-dsaa/blob/master/02-MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae284be",
      "metadata": {
        "id": "4ae284be"
      },
      "source": [
        "# scikit-learn, Machine Learning in Python\n",
        "- Simple and efficient tools for predictive data analysis\n",
        "- Accessible to everybody, and reusable in various contexts\n",
        "- Built on NumPy, SciPy, and matplotlib\n",
        "- Open source, commercially usable - BSD license\n",
        "\n",
        "\n",
        "Check also: \n",
        "- https://scikit-learn.org/stable/index.html\n",
        "- https://scikit-learn.org/stable/modules/tree.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3932b785",
      "metadata": {
        "scrolled": false,
        "id": "3932b785"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a45a8f4",
      "metadata": {
        "id": "0a45a8f4"
      },
      "source": [
        "Solution of the previous `Hands on`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87eb7c60",
      "metadata": {
        "id": "87eb7c60"
      },
      "outputs": [],
      "source": [
        "def preprocess(normalize=True):\n",
        "    # df = pd.read_csv(\"datasets/2022-bbs-dsaa-housing.csv\", delimiter=\",\")\n",
        "    df = pd.read_csv(\"http://big.csr.unibo.it/projects/nosql-datasets/2022-bbs-dsaa-housing.csv\", delimiter=\",\")\n",
        "    \n",
        "    num_df = df.drop(columns=[\"ocean_proximity\", \"median_house_value\"])\n",
        "\n",
        "    # Filling in (i.e., impute) missing values with the median value \n",
        "    num_df[\"total_bedrooms\"] = num_df[\"total_bedrooms\"].fillna(num_df[\"total_bedrooms\"].median())\n",
        "\n",
        "    # Add a new column: population_per_household = population / households\n",
        "    num_df[\"population_per_household\"] = num_df[\"population\"] / num_df[\"households\"]\n",
        "\n",
        "    # Add a new column: rooms_per_household = total_rooms / households\n",
        "    num_df[\"rooms_per_household\"] = num_df[\"total_rooms\"] / num_df[\"households\"]\n",
        "\n",
        "    # Add a new column: bedrooms_per_room = total_bedrooms / total_rooms\n",
        "    num_df[\"bedrooms_per_room\"] = num_df[\"total_bedrooms\"] / num_df[\"total_rooms\"]\n",
        "\n",
        "    if normalize:\n",
        "        # Apply standardization to all the numeric columns\n",
        "        num_df = (num_df - num_df.mean()) / num_df.std()\n",
        "\n",
        "    # One hot encode `ocean_proximity` since it is a categorical attribute \n",
        "    cat_df = pd.get_dummies(df[\"ocean_proximity\"], prefix='ocean_proximity')\n",
        "\n",
        "    # Join all the dataframes\n",
        "    return pd.concat([num_df, cat_df, df[[\"median_house_value\"]]], axis=1) # do not change this line\n",
        "\n",
        "df = preprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc20801",
      "metadata": {
        "id": "1dc20801"
      },
      "source": [
        "For a supervised learning problem we need:\n",
        "- input data along with labels\n",
        "- split data between test and training set. How?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a9526a",
      "metadata": {
        "id": "21a9526a"
      },
      "outputs": [],
      "source": [
        "# For illustration only. Sklearn has train_test_split()\n",
        "def split_train_test(data, test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data)) # get a list of random numbers\n",
        "    test_set_size = int(len(data) * test_ratio) # define the size of test dataset\n",
        "    test_indices = shuffled_indices[:test_set_size] # the test dataset includes some indexes \n",
        "    train_indices = shuffled_indices[test_set_size:] # the train dataset includes the others (no overlapping)\n",
        "    return data.iloc[train_indices], data.iloc[test_indices] # return the two dataframes\n",
        "\n",
        "df_train, df_test = split_train_test(df, 0.2) # get the train and test dataframes\n",
        "\n",
        "X_train = df_train.drop(columns=[\"median_house_value\"]).to_numpy() # get the train feature matrix\n",
        "X_test  = df_test.drop(columns=[\"median_house_value\"]).to_numpy() # get the test feature matrix\n",
        "y_train = df_train[\"median_house_value\"].to_numpy() # get the train label array\n",
        "y_test  = df_test[\"median_house_value\"].to_numpy() # get the test label array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fe3dce",
      "metadata": {
        "id": "09fe3dce"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df[\"median_house_value\"] # labels\n",
        "X = df.drop(columns=[\"median_house_value\"]) # input data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613643ea",
      "metadata": {
        "id": "613643ea"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456e291b",
      "metadata": {
        "id": "456e291b"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90facd0d",
      "metadata": {
        "id": "90facd0d"
      },
      "source": [
        "Apply linear regression to forecast housing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805e7157",
      "metadata": {
        "id": "805e7157"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression # import the model\n",
        "\n",
        "lin_reg = LinearRegression() # initialize the model (i.e., the estimator)\n",
        "lin_reg.fit(X_train, y_train) # train it\n",
        "housing_predictions = lin_reg.predict(X_test) # predict the cost of houses in the test set\n",
        "\n",
        "lin_mse = mean_squared_error(y_test, housing_predictions) # check the error\n",
        "lin_rmse = np.sqrt(lin_mse) # apply the squared root of the error\n",
        "print(lin_rmse) # print it\n",
        "\n",
        "# visualize some predictions\n",
        "df = pd.DataFrame({'y_test': y_test[:10].to_numpy(), 'y_pred': housing_predictions[:10]}, index=[x for x in range(10)])\n",
        "df.plot.bar(rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428764a9",
      "metadata": {
        "id": "428764a9"
      },
      "source": [
        "This is better than nothing, but clearly not a great score: most districtsâ€™ `median_housing_values` range between 120K USD and  265K USD, so a typical prediction error of ~70K USD is not very satisfying. This is an example of a model underfitting the training\n",
        "data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109abc76",
      "metadata": {
        "id": "109abc76"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor # import the model\n",
        "\n",
        "def run_forest(n_estimators, max_features):\n",
        "    # initialize the model (i.e., the estimator)\n",
        "    forest_reg = RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, random_state=42)\n",
        "    forest_reg.fit(X_train, y_train) # train it\n",
        "    housing_predictions = forest_reg.predict(X_test) # predict the cost of houses in the test set\n",
        "    \n",
        "    forest_rmse = mean_squared_error(y_test, housing_predictions, squared=True) # check the error\n",
        "    forest_rmse = np.sqrt(forest_rmse) # apply the squared root of the error\n",
        "    print(forest_rmse)\n",
        "    \n",
        "    df = pd.DataFrame({'y_test': y_test[:10].to_numpy(), 'y_pred': housing_predictions[:10]}, index=[x for x in range(10)])\n",
        "    df.plot.bar(rot=0)\n",
        "    return forest_reg\n",
        "\n",
        "forest_reg = run_forest(100, \"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847f5256",
      "metadata": {
        "id": "847f5256"
      },
      "source": [
        "Look at parameters used by our current forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0235826",
      "metadata": {
        "id": "f0235826"
      },
      "outputs": [],
      "source": [
        "forest_reg.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d94f505",
      "metadata": {
        "scrolled": true,
        "id": "8d94f505"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "\n",
        "n_estimators = [50, 100, 150] # Number of trees in random forest\n",
        "max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
        "# auto -> n_features\n",
        "# sqrt -> max_features=sqrt(n_features)\n",
        "random_grid = {'n_estimators': n_estimators, 'max_features': max_features}\n",
        "\n",
        "# Fit the random search model\n",
        "rf_random = RandomizedSearchCV(estimator = forest_reg, param_distributions = random_grid, cv = 3, verbose=2, random_state=42)\n",
        "rf_random.fit(X_train, y_train)\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40f18a9",
      "metadata": {
        "id": "f40f18a9"
      },
      "outputs": [],
      "source": [
        "run_forest(150, \"sqrt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f03591",
      "metadata": {
        "id": "47f03591"
      },
      "source": [
        "#### Dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41167ddd",
      "metadata": {
        "scrolled": true,
        "id": "41167ddd"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X_train.values)\n",
        "\n",
        "# tsne = TSNE(n_components=2)\n",
        "# result = tsne.fit_transform(X_train)\n",
        "\n",
        "plt.scatter(\n",
        "    x=result[:,0], \n",
        "    y=result[:,1] , \n",
        "    c=y_train, #.apply(lambda x: 0 if x == 'low' else (1 if x == 'medium' else 2)), \n",
        "    cmap='viridis'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b18f9db",
      "metadata": {
        "scrolled": true,
        "id": "3b18f9db"
      },
      "outputs": [],
      "source": [
        "# Dump components relations with features\n",
        "pd.DataFrame(pca.components_, columns=X_train.columns, index=['PC1', 'PC2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d65e48c",
      "metadata": {
        "id": "7d65e48c"
      },
      "source": [
        "#### Classification\n",
        "\n",
        "- Create classes (low, medium, high) from the target value\n",
        "- Create and fit a decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16c0883",
      "metadata": {
        "scrolled": false,
        "id": "f16c0883"
      },
      "outputs": [],
      "source": [
        "df = preprocess(False) # clean the dataset withtout standardization\n",
        "bins = [\"low\", \"medium\", \"high\"] # the labels we are gonna use\n",
        "df[\"median_house_value\"] = pd.cut(df[\"median_house_value\"], 3, labels=bins) # map bins of values into labels\n",
        "df[\"median_house_value\"].hist() # plot them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a864ea4",
      "metadata": {
        "id": "0a864ea4"
      },
      "source": [
        "For the sake of performance, we are going to use a manually defined set of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafc83b9",
      "metadata": {
        "id": "eafc83b9"
      },
      "outputs": [],
      "source": [
        "y = df[\"median_house_value\"] # labels\n",
        "X = df[[\"latitude\", \"longitude\", \"median_income\", \"population\", \"households\", \"total_bedrooms\"]] # consider only some features from the train dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a910d0",
      "metadata": {
        "id": "f6a910d0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=42) # initialize the model\n",
        "clf = clf.fit(X_train, y_train) # train it\n",
        "housing_predictions = clf.predict(X_test) # predict the cost of houses in the test set\n",
        "\n",
        "accuracy = accuracy_score(y_test, housing_predictions) # check the error\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a816a7f",
      "metadata": {
        "id": "0a816a7f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, housing_predictions, target_names=bins))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "017f3c68",
      "metadata": {
        "scrolled": true,
        "id": "017f3c68"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "tree.plot_tree(clf, fontsize=10, feature_names=X_train.columns, class_names=bins, max_depth=4)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}