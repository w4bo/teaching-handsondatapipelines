{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4Kv3hkDxKzO5dqbzLV2n1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w4bo/handsOnDataPipelines/blob/main/materials/03-BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQZA_5DJpP0w"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!wget http://big.csr.unibo.it/projects/nosql-datasets/2022-bbs-dsaa-foodmart.csv\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "wKdNPl5Cp8HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "rxvFzMF3piKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create a simple example\n",
        "riddle = \"sopra la panca la capra campa sotto la panca la capra crepa\""
      ],
      "metadata": {
        "id": "gmVO4Ctutr-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "rdd = sc.parallelize(riddle.split(\" \"))  # create an RDD from the `riddle` string\n",
        "# each tuple of the RDD corresponds to a single word\n",
        "# why is an RDD and not its result returned?\n",
        "rdd"
      ],
      "metadata": {
        "id": "Tjy8IlWut75A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of action\n",
        "rdd.collect()"
      ],
      "metadata": {
        "id": "nvm762YDuO6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of transformations\n",
        "# - transform each string in upper case (remember: map returns a new RDD with the same cardinality)\n",
        "# - keep only the strings beginning with \"C\" (remember: filter returns a new RDD with the same or smaller cardinality)\n",
        "# - explode each string into its characters (remember: flatMap returns a new RDD with the any cardinality)\n",
        "rdd \\\n",
        "    .map(lambda s: s.upper()) \\\n",
        "    .filter(lambda s: s.startswith(\"C\")) \\\n",
        "    .flatMap(lambda s: list(s)) \\\n",
        "    .collect()"
      ],
      "metadata": {
        "id": "Zt0nJGqIuRk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple word count\n",
        "# - map each word to a tuple (word, 1); each tuple represent the count associate with a word\n",
        "# - group all the tuples with the same word and sum the counts\n",
        "# - sort tuples by count\n",
        "# - get the values\n",
        "rdd \\\n",
        "    .map(lambda s: (s, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: x[1]) \\\n",
        "    .collect() "
      ],
      "metadata": {
        "id": "beFd4AOdvbVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"2022-bbs-dsaa-foodmart.csv\", header=True, sep=\",\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "zGXci7gVqPVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "# Import the data\n",
        "# - read the file\n",
        "# - which has a csv (comma separated value) format\n",
        "# - select some of its columns\n",
        "# - cache the RDD\n",
        "df = spark\\\n",
        "          .read \\\n",
        "          .csv(\"2022-bbs-dsaa-foodmart.csv\", header=True, sep=\",\") \\\n",
        "          .select(col('Product (Category)').alias('product'), col(\"subcategory\"), col(\"category\"), col(\"unit sales\").cast(\"int\")) \\\n",
        "          .cache()\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "PvR9xLqcrEN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show only the sales for category = 'Pizza'\n",
        "df \\\n",
        "    .filter(\"category = 'Pizza'\") \\\n",
        "    .show(20, False)"
      ],
      "metadata": {
        "id": "NK6DuAZ2soMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count all the product sales\n",
        "df.count()"
      ],
      "metadata": {
        "id": "YLPJtBensukb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the distinct products\n",
        "# - select the products only\n",
        "# - get the distinct values\n",
        "# - and count them\n",
        "df.select(\"product\") \\\n",
        "  .distinct() \\\n",
        "  .count() "
      ],
      "metadata": {
        "id": "daV74a7ytEJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the number of products with category = 'Pizza'\n",
        "# - filter pizzas\n",
        "# - select the products only\n",
        "# - get the distinct values\n",
        "# - and count them\n",
        "df \\\n",
        "    .filter(\"category = 'Pizza'\") \\\n",
        "    .select(\"product\") \\\n",
        "    .distinct() \\\n",
        "    .count() "
      ],
      "metadata": {
        "id": "NCr1g9WttLro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the average sales by category\n",
        "# - select the products only\n",
        "# - average the unit sales\n",
        "df \\\n",
        "    .groupBy(\"category\") \\\n",
        "    .avg(\"unit sales\") \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "epSAiA_VtYix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}